{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnect:\n",
    "    def __init__(self, l_x, l_y):  # 两个参数分别为输入层的长度和输出层的长度\n",
    "        # 使用随机数初始化参数，请暂时忽略这里为什么多了np.sqrt(l_x)\n",
    "        self.weights = np.random.randn(l_y, l_x) / np.sqrt(l_x)\n",
    "        self.bias = np.random.randn(l_y, 1)  # 使用随机数初始化参数\n",
    "        self.lr = 0  # 先将学习速率初始化为0，最后统一设置学习速率\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  # 把中间结果保存下来，以备反向传播时使用\n",
    "        self.y = np.array([np.dot(self.weights, xx) +\n",
    "                           self.bias for xx in x])  # 计算全连接层的输出\n",
    "#         print('FullyConnect np.array([np.dot(self.weights, xx) + self.bias for xx in x]).shape', np.array([np.dot(self.weights, xx) + self.bias for xx in x]).shape)\n",
    "        return self.y  # 将这一层计算的结果向前传递\n",
    "\n",
    "    def backward(self, d):\n",
    "        # 根据链式法则，将反向传递回来的导数值乘以x，得到对参数的梯度\n",
    "        ddw = [np.dot(dd, xx.T) for dd, xx in zip(d, self.x)]\n",
    "        # 每一条数据都能求出一个ddw，然后对他们取一个平均，得到平均的梯度变化\n",
    "        self.dw = np.sum(ddw, axis=0) / self.x.shape[0]\n",
    "        self.db = np.sum(d, axis=0) / self.x.shape[0]\n",
    "        self.dx = np.array([np.dot(self.weights.T, dd) for dd in d])\n",
    "\n",
    "        # 利用梯度下降的思想，更新参数。这里的lr就是步长的意思\n",
    "        self.weights -= self.lr * self.dw\n",
    "        self.bias -= self.lr * self.db\n",
    "        return self.dx  # 反向传播梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):  # 无参数，不需初始化\n",
    "        pass\n",
    "    # 这里输入的变量的 x，其实就是上面公式的 z\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # 完成正向传播，将输入的 z ，放入 Sigmoid 函数中，最终得到结果 h，并返回\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.sigmoid(x)\n",
    "        return self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, label):  # 只需forward\n",
    "        self.accuracy = np.sum(\n",
    "            [np.argmax(xx) == ll for xx, ll in zip(x, label)])  # 对预测正确的实例数求和\n",
    "        self.accuracy = 1.0 * self.accuracy / x.shape[0]  # 也就是计算正确率 ,公式 7 的实现\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # 正向传播和上文一样，具体注释参照上文\n",
    "    def forward(self, x, label):\n",
    "        self.x = x\n",
    "        self.label = np.zeros_like(x)\n",
    "        for a, b in zip(self.label, label):\n",
    "            a[b] = 1.0\n",
    "        # 对公式 8 实现\n",
    "        self.loss = np.sum(np.square(x - self.label)) / \\\n",
    "        self.x.shape[0] / 2  # 求平均后再除以2是为了表示方便\n",
    "        return self.loss\n",
    "\n",
    "    # 定义反向传播\n",
    "    def backward(self):\n",
    "        # 这里的dx，就是我们求得函数关于x偏导数，也就是梯度，将它保存起来，后面更新的时候会用到\n",
    "        self.dx = (self.x - self.label) / self.x.shape[0]  # 2被抵消掉了\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 21.790288505475054 accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 图片大小为 8*8\n",
    "# 则此时一张图片就是一条数据，每张图片对呀一个 label（0-9范围内）\n",
    "x = digits.data\n",
    "labels = digits.target\n",
    "\n",
    "# 开始搭建神经网络\n",
    "inner_layers = []\n",
    "inner_layers.append(FullyConnect(8 * 8, 10))\n",
    "inner_layers.append(Sigmoid())\n",
    "# 神经网络搭建完成\n",
    "\n",
    "losslayer = QuadraticLoss()  # 计算损失\n",
    "accuracy = Accuracy()  # 计算准确率\n",
    "\n",
    "# 开始将数据送入神经网络进行正向传播\n",
    "for layer in inner_layers:  # 前向计算\n",
    "    x = layer.forward(x)\n",
    "\n",
    "loss = losslayer.forward(x, labels)  # 调用损失层forward函数计算损失函数值\n",
    "accu = accuracy.forward(x, labels)\n",
    "print('loss:', loss, 'accuracy:', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):  # 无参数，不需初始化\n",
    "        pass\n",
    "    # 即公式 5\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.sigmoid(x)\n",
    "        return self.y\n",
    "    # 即公式 9\n",
    "    def backward(self, d):\n",
    "        sig = self.sigmoid(self.x)\n",
    "        self.dx = d * sig * (1 - sig)\n",
    "        return self.dx  # 反向传递梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 64), (1500,), (296, 64), (296,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data,train_target = digits.data[:1500],digits.target[:1500]\n",
    "test_data,test_target = digits.data[1500:-1],digits.target[1500:-1]\n",
    "train_data.shape,train_target.shape,test_data.shape,test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.FullyConnect at 0x2baf0b0db50>,\n",
       " <__main__.Sigmoid at 0x2baf0b0d5b0>,\n",
       " <__main__.FullyConnect at 0x2baf0b0da00>,\n",
       " <__main__.Sigmoid at 0x2baf0b0db80>,\n",
       " <__main__.FullyConnect at 0x2baf0b0d4f0>,\n",
       " <__main__.Sigmoid at 0x2baf0b0daf0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_layers = []\n",
    "layer1 = FullyConnect(64, 32)\n",
    "inner_layers.append(layer1) # 因为每条数据的长度为 8*8=64，因此这里第一个全连接层，接收长度为64\n",
    "inner_layers.append(Sigmoid())\n",
    "inner_layers.append(FullyConnect(32, 16)) # 因为每条数据的长度为 8*8=64，因此这里第一个全连接层，接收长度为64\n",
    "inner_layers.append(Sigmoid())\n",
    "inner_layers.append(FullyConnect(16, 10))\n",
    "inner_layers.append(Sigmoid())\n",
    "inner_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.QuadraticLoss at 0x2baf0b0d760>,\n",
       " <__main__.Accuracy at 0x2baf0b0de80>,\n",
       " 350)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losslayer = QuadraticLoss()\n",
    "accuracy = Accuracy()\n",
    "for layer in inner_layers:\n",
    "    layer.lr = 2000     #所有中间层设置学习速率\n",
    "epochs = 350  # 对训练数据遍历的次数，也就是学习时间。\n",
    "#在开始的时候，准确率会随之学习时间的增加而提高。\n",
    "#当模型学习完训练数据中的所有信息后，准确率就会趋于稳定\n",
    "losslayer,accuracy,epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:0,loss:1.7851388233987988,test_accuracy:0.10135135135135136\n",
      "epochs:10,loss:0.4485045183610696,test_accuracy:0.2533783783783784\n",
      "epochs:20,loss:0.4452012969483326,test_accuracy:0.33783783783783783\n",
      "epochs:30,loss:0.4428102113599457,test_accuracy:0.30743243243243246\n",
      "epochs:40,loss:0.43991951108548816,test_accuracy:0.32094594594594594\n",
      "epochs:50,loss:0.4366099803506653,test_accuracy:0.3344594594594595\n",
      "epochs:60,loss:0.43273574486054195,test_accuracy:0.3783783783783784\n",
      "epochs:70,loss:0.4280262464050834,test_accuracy:0.4222972972972973\n",
      "epochs:80,loss:0.4227234454506997,test_accuracy:0.44594594594594594\n",
      "epochs:90,loss:0.41646655058200577,test_accuracy:0.46621621621621623\n",
      "epochs:100,loss:0.40859995818705813,test_accuracy:0.4831081081081081\n",
      "epochs:110,loss:0.3991420234540602,test_accuracy:0.5067567567567568\n",
      "epochs:120,loss:0.38806232220141934,test_accuracy:0.5202702702702703\n",
      "epochs:130,loss:0.3752889649102913,test_accuracy:0.5337837837837838\n",
      "epochs:140,loss:0.36075885404393476,test_accuracy:0.5608108108108109\n",
      "epochs:150,loss:0.34533666361368737,test_accuracy:0.5641891891891891\n",
      "epochs:160,loss:0.32929586409209755,test_accuracy:0.5844594594594594\n",
      "epochs:170,loss:0.3128855470032763,test_accuracy:0.6081081081081081\n",
      "epochs:180,loss:0.29648391801761514,test_accuracy:0.6418918918918919\n",
      "epochs:190,loss:0.2804319465961088,test_accuracy:0.6587837837837838\n",
      "epochs:200,loss:0.264913361056723,test_accuracy:0.6722972972972973\n",
      "epochs:210,loss:0.25005891370213823,test_accuracy:0.6891891891891891\n",
      "epochs:220,loss:0.23591571821693674,test_accuracy:0.7364864864864865\n",
      "epochs:230,loss:0.22249101148546158,test_accuracy:0.7533783783783784\n",
      "epochs:240,loss:0.20979029366491264,test_accuracy:0.7635135135135135\n",
      "epochs:250,loss:0.19782617020181964,test_accuracy:0.793918918918919\n",
      "epochs:260,loss:0.18662890271250318,test_accuracy:0.8006756756756757\n",
      "epochs:270,loss:0.17620658555761995,test_accuracy:0.8006756756756757\n",
      "epochs:280,loss:0.16653470290594286,test_accuracy:0.8040540540540541\n",
      "epochs:290,loss:0.15760630388917352,test_accuracy:0.8006756756756757\n",
      "epochs:300,loss:0.14939985160051678,test_accuracy:0.8074324324324325\n",
      "epochs:310,loss:0.1418542980168944,test_accuracy:0.8108108108108109\n",
      "epochs:320,loss:0.13490667635710255,test_accuracy:0.8175675675675675\n",
      "epochs:330,loss:0.12848985314848613,test_accuracy:0.8243243243243243\n",
      "epochs:340,loss:0.12253285282033835,test_accuracy:0.8277027027027027\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    losssum = 0\n",
    "    iters = 0\n",
    "    x = train_data\n",
    "    label = train_target\n",
    "    x = x.reshape(-1,64,1)\n",
    "    for layer in inner_layers:  # 前向计算\n",
    "        x = layer.forward(x)\n",
    "    loss = losslayer.forward(x, label)  # 调用损失层forward函数计算损失函数值\n",
    "    losssum += loss\n",
    "    iters += 1\n",
    "    d = losslayer.backward()  # 调用损失层backward函数层计算将要反向传播的梯度\n",
    "\n",
    "    for layer in inner_layers[::-1]:  # 反向传播\n",
    "        d = layer.backward(d)\n",
    "\n",
    "    if i%10==0: \n",
    "        x = test_data\n",
    "        label = test_target\n",
    "        x = x.reshape(-1,64,1)\n",
    "        for layer in inner_layers:\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        accu = accuracy.forward(x, label)  # 调用准确率层forward()函数求出准确率\n",
    "        print('epochs:{},loss:{},test_accuracy:{}'.format(i,losssum / iters,accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 64, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
