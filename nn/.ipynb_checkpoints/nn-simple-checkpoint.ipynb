{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnect:\n",
    "    def __init__(self, l_x, l_y):  # 两个参数分别为输入层的长度和输出层的长度\n",
    "        # 使用随机数初始化参数，请暂时忽略这里为什么多了np.sqrt(l_x)\n",
    "        self.weights = np.random.randn(l_y, l_x) / np.sqrt(l_x)\n",
    "        self.bias = np.random.randn(l_y, 1)  # 使用随机数初始化参数\n",
    "        self.lr = 0  # 先将学习速率初始化为0，最后统一设置学习速率\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  # 把中间结果保存下来，以备反向传播时使用\n",
    "        self.y = np.array([np.dot(self.weights, xx) +\n",
    "                           self.bias for xx in x])  # 计算全连接层的输出\n",
    "#         print('FullyConnect np.array([np.dot(self.weights, xx) + self.bias for xx in x]).shape', np.array([np.dot(self.weights, xx) + self.bias for xx in x]).shape)\n",
    "        return self.y  # 将这一层计算的结果向前传递\n",
    "\n",
    "    def backward(self, d):\n",
    "        # 根据链式法则，将反向传递回来的导数值乘以x，得到对参数的梯度\n",
    "        ddw = [np.dot(dd, xx.T) for dd, xx in zip(d, self.x)]\n",
    "        # 每一条数据都能求出一个ddw，然后对他们取一个平均，得到平均的梯度变化\n",
    "        self.dw = np.sum(ddw, axis=0) / self.x.shape[0]\n",
    "        self.db = np.sum(d, axis=0) / self.x.shape[0]\n",
    "        self.dx = np.array([np.dot(self.weights.T, dd) for dd in d])\n",
    "\n",
    "        # 利用梯度下降的思想，更新参数。这里的lr就是步长的意思\n",
    "        self.weights -= self.lr * self.dw\n",
    "        self.bias -= self.lr * self.db\n",
    "        return self.dx  # 反向传播梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):  # 无参数，不需初始化\n",
    "        pass\n",
    "    # 这里输入的变量的 x，其实就是上面公式的 z\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # 完成正向传播，将输入的 z ，放入 Sigmoid 函数中，最终得到结果 h，并返回\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.sigmoid(x)\n",
    "        return self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, label):  # 只需forward\n",
    "        self.accuracy = np.sum(\n",
    "            [np.argmax(xx) == ll for xx, ll in zip(x, label)])  # 对预测正确的实例数求和\n",
    "        self.accuracy = 1.0 * self.accuracy / x.shape[0]  # 也就是计算正确率 ,公式 7 的实现\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # 正向传播和上文一样，具体注释参照上文\n",
    "    def forward(self, x, label):\n",
    "        self.x = x\n",
    "        self.label = np.zeros_like(x)\n",
    "        for a, b in zip(self.label, label):\n",
    "            a[b] = 1.0\n",
    "        # 对公式 8 实现\n",
    "        self.loss = np.sum(np.square(x - self.label)) / \\\n",
    "        self.x.shape[0] / 2  # 求平均后再除以2是为了表示方便\n",
    "        return self.loss\n",
    "\n",
    "    # 定义反向传播\n",
    "    def backward(self):\n",
    "        # 这里的dx，就是我们求得函数关于x偏导数，也就是梯度，将它保存起来，后面更新的时候会用到\n",
    "        self.dx = (self.x - self.label) / self.x.shape[0]  # 2被抵消掉了\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片大小为 8*8\n",
    "# 则此时一张图片就是一条数据，每张图片对呀一个 label（0-9范围内）\n",
    "x = digits.data\n",
    "labels = digits.target\n",
    "\n",
    "# 开始搭建神经网络\n",
    "inner_layers = []\n",
    "inner_layers.append(FullyConnect(8 * 8, 10))\n",
    "inner_layers.append(Sigmoid())\n",
    "# 神经网络搭建完成\n",
    "\n",
    "losslayer = QuadraticLoss()  # 计算损失\n",
    "accuracy = Accuracy()  # 计算准确率\n",
    "\n",
    "# 开始将数据送入神经网络进行正向传播\n",
    "for layer in inner_layers:  # 前向计算\n",
    "    x = layer.forward(x)\n",
    "\n",
    "loss = losslayer.forward(x, labels)  # 调用损失层forward函数计算损失函数值\n",
    "accu = accuracy.forward(x, labels)\n",
    "print('loss:', loss, 'accuracy:', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):  # 无参数，不需初始化\n",
    "        pass\n",
    "    # 即公式 5\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.sigmoid(x)\n",
    "        return self.y\n",
    "    # 即公式 9\n",
    "    def backward(self, d):\n",
    "        sig = self.sigmoid(self.x)\n",
    "        self.dx = d * sig * (1 - sig)\n",
    "        return self.dx  # 反向传递梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_target = digits.data[:1500],digits.target[:1500]\n",
    "test_data,test_target = digits.data[1500:-1],digits.target[1500:-1]\n",
    "train_data.shape,train_target.shape,test_data.shape,test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_layers = []\n",
    "inner_layers.append(FullyConnect(64, 32)) # 因为每条数据的长度为 8*8=64，因此这里第一个全连接层，接收长度为64\n",
    "inner_layers.append(Sigmoid())\n",
    "inner_layers.append(FullyConnect(32, 16)) # 因为每条数据的长度为 8*8=64，因此这里第一个全连接层，接收长度为64\n",
    "inner_layers.append(Sigmoid())\n",
    "inner_layers.append(FullyConnect(16, 10))\n",
    "inner_layers.append(Sigmoid())\n",
    "inner_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losslayer = QuadraticLoss()\n",
    "accuracy = Accuracy()\n",
    "for layer in inner_layers:\n",
    "    layer.lr = 1000     #所有中间层设置学习速率\n",
    "epochs = 350  # 对训练数据遍历的次数，也就是学习时间。\n",
    "#在开始的时候，准确率会随之学习时间的增加而提高。\n",
    "#当模型学习完训练数据中的所有信息后，准确率就会趋于稳定\n",
    "losslayer,accuracy,epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    losssum = 0\n",
    "    iters = 0\n",
    "    x = train_data\n",
    "    label = train_target\n",
    "    x = x.reshape(-1,64,1)\n",
    "    for layer in inner_layers:  # 前向计算\n",
    "        x = layer.forward(x)\n",
    "    loss = losslayer.forward(x, label)  # 调用损失层forward函数计算损失函数值\n",
    "    losssum += loss\n",
    "    iters += 1\n",
    "    d = losslayer.backward()  # 调用损失层backward函数层计算将要反向传播的梯度\n",
    "\n",
    "    for layer in inner_layers[::-1]:  # 反向传播\n",
    "        d = layer.backward(d)\n",
    "\n",
    "    if i%10==0: \n",
    "        x = test_data\n",
    "        label = test_target\n",
    "        x = x.reshape(-1,64,1)\n",
    "        for layer in inner_layers:\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        accu = accuracy.forward(x, label)  # 调用准确率层forward()函数求出准确率\n",
    "        print('epochs:{},loss:{},test_accuracy:{}'.format(i,losssum / iters,accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
